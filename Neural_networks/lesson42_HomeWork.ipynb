{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595ac4a8-e1d7-4261-979b-14b9f2ed5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c586f2f-4d22-4ef1-a59b-b999039136ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df_pikabu_spam_posts.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927b161a-c656-4fb6-82b5-d00b96dc54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6408f013-bead-4332-8681-b080ab7bcbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All words: 59073\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for word in row.title:\n",
    "        words_set.add(word)\n",
    "    for word in row.text:\n",
    "        words_set.add(word)\n",
    "        \n",
    "print(f'All words: {len(words_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e8a7e3-b22a-476e-ae4b-1ddf574ffccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counter = {w: 0 for w  in words_set}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for word in row.title:\n",
    "        words_counter[word] += 1\n",
    "    for word in row.text:\n",
    "        words_counter[word] += 1\n",
    "        \n",
    "word_list = list(words_counter.items())\n",
    "word_list.sort(key=(lambda x: x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac9dbdc-01ab-4bcd-946e-754208f5832b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('эт', 11379),\n",
       " ('котор', 6957),\n",
       " ('сво', 4743),\n",
       " ('год', 4251),\n",
       " ('так', 4122),\n",
       " ('сам', 3757),\n",
       " ('одн', 3251),\n",
       " ('работ', 3207),\n",
       " ('очен', 3167),\n",
       " ('прост', 3116)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3b9d9e-4872-47c8-ab1e-8c938572493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = word_list[:5000]\n",
    "\n",
    "word_list = [k[0] for k in word_list]\n",
    "\n",
    "words_ohe_position = {word_list[i]: i for i in range(len(word_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a6bfad-ed7a-4ad3-816b-9fb0a1915e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "texts = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    title_ohe = [0]*len(word_list)\n",
    "    for word in row.title:\n",
    "        try:\n",
    "            title_ohe[words_ohe_position[word]] += 1\n",
    "        except:\n",
    "            continue\n",
    "    text_ohe = [0]*len(word_list)\n",
    "    for word in row.text:\n",
    "        try:\n",
    "            text_ohe[words_ohe_position[word]] += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    titles.append(title_ohe)\n",
    "    texts.append(text_ohe)\n",
    "    \n",
    "titles = np.array(titles)\n",
    "texts = np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0be302-8726-4057-bf62-ec4f429e9405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7443, 5000), (7443, 5000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.shape,  texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3efc012-75da-4d87-9e0d-e561fa463d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a122b5b-d718-4b92-94c5-8fc6bd590b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(x, val_frac=0.15, test_frac=0.15):\n",
    "    x_train = x[:round((1-val_frac-test_frac)*len(x))]\n",
    "    x_val = x[round((1-val_frac-test_frac)*len(x)):round((1-test_frac)*len(x))]\n",
    "    x_test = x[round((1-test_frac)*len(x)):]\n",
    "    return x_train, x_val, x_test\n",
    "\n",
    "titles_train, titles_val, titles_test = train_val_test_split(titles)\n",
    "texts_train, texts_val, texts_test = train_val_test_split(texts)\n",
    "y_train, y_val, y_test = train_val_test_split(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01519cad-c645-4b20-bd75-2e0e72c9ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(5000, ))\n",
    "\n",
    "text_dense_1 = tf.keras.layers.Dense(500, activation='relu', use_bias=True)(text_input)\n",
    "\n",
    "text_bn_1 = tf.keras.layers.BatchNormalization(center=True, scale=True)(text_dense_1)\n",
    "\n",
    "text_dense_2 = tf.keras.layers.Dense(500, activation='relu', use_bias=True)(text_bn_1)\n",
    "\n",
    "text_bn_2 = tf.keras.layers.BatchNormalization(center=True, scale=True)(text_dense_2)\n",
    "\n",
    "title_input = tf.keras.layers.Input(shape=(5000, ))\n",
    "\n",
    "title_dense_1 = tf.keras.layers.Dense(500, activation='relu', use_bias=True)(title_input)\n",
    "\n",
    "title_bn_1 = tf.keras.layers.BatchNormalization(center=True, scale=True)(title_dense_1)\n",
    "\n",
    "concat = tf.keras.layers.Concatenate()([title_bn_1, text_bn_2])\n",
    "\n",
    "main_dense_1 = tf.keras.layers.Dense(300, activation='relu', use_bias=True)(concat)\n",
    "\n",
    "main_bn_1 = tf.keras.layers.BatchNormalization(center=True, scale=True)(main_dense_1) \n",
    "\n",
    "drop_main_1 = tf.keras.layers.Dropout(0.8)(main_bn_1)\n",
    "\n",
    "main_dense_2 = tf.keras.layers.Dense(100, activation='relu', use_bias=True)(drop_main_1)\n",
    "\n",
    "main_bn_2 = tf.keras.layers.BatchNormalization(center=True, scale=True)(main_dense_2)\n",
    "\n",
    "drop_main_2 = tf.keras.layers.Dropout(0.8)(main_bn_2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(drop_main_2)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=[title_input, text_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1975ddbb-f741-421e-87c9-e2b1a6bfb8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 5000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 500)          2500500     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 5000)]       0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 500)         2000        ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 500)          2500500     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 500)          250500      ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 500)         2000        ['dense_8[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 500)         2000        ['dense_7[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1000)         0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 300)          300300      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 300)         1200        ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 300)          0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 100)          30100       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 100)         400         ['dense_10[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100)          0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            101         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,589,601\n",
      "Trainable params: 5,585,801\n",
      "Non-trainable params: 3,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367fb733-6369-4510-a037-042db687731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 4s 140ms/step - loss: 1.1746 - binary_accuracy: 0.5344 - val_loss: 0.5891 - val_binary_accuracy: 0.7144\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 0.8061 - binary_accuracy: 0.6393 - val_loss: 0.5444 - val_binary_accuracy: 0.7431\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 3s 124ms/step - loss: 0.5505 - binary_accuracy: 0.7522 - val_loss: 0.4808 - val_binary_accuracy: 0.8424\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 3s 129ms/step - loss: 0.3544 - binary_accuracy: 0.8543 - val_loss: 0.3910 - val_binary_accuracy: 0.9150\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 3s 125ms/step - loss: 0.2367 - binary_accuracy: 0.9234 - val_loss: 0.3227 - val_binary_accuracy: 0.9257\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 3s 125ms/step - loss: 0.1661 - binary_accuracy: 0.9532 - val_loss: 0.2470 - val_binary_accuracy: 0.9284\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 3s 124ms/step - loss: 0.1073 - binary_accuracy: 0.9752 - val_loss: 0.2148 - val_binary_accuracy: 0.9320\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 3s 128ms/step - loss: 0.0850 - binary_accuracy: 0.9818 - val_loss: 0.1894 - val_binary_accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 3s 126ms/step - loss: 0.0534 - binary_accuracy: 0.9937 - val_loss: 0.1834 - val_binary_accuracy: 0.9320\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 3s 133ms/step - loss: 0.0432 - binary_accuracy: 0.9919 - val_loss: 0.1758 - val_binary_accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a442211c00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=[tf.keras.metrics.binary_accuracy]) \n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/second/', histogram_freq=1)\n",
    "\n",
    "model.fit([titles_train, texts_train], y_train,\n",
    "          validation_data=([titles_val, texts_val], y_val),\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          callbacks=[tb_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e3599-ad79-40d7-8f99-fc9b07d9aab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
